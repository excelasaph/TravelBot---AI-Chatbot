experiment_id,model_architecture,epochs,batch_size,learning_rate,warmup_steps,weight_decay,rouge1,rouge2,rougeL,rougeLsum,bertscore_precision,bertscore_recall,bertscore_f1,bleu_score,notes
1,t5-base,3,8,5e-5,500,0.01,0.3814,0.1824,0.2958,0.2989,0.8893,0.8647,0.8765,0.1088,"Final selected model (best overall performance)"
2,t5-base,2,8,5e-5,500,0.01,0.3612,0.1721,0.2801,0.2845,0.8712,0.8534,0.8621,0.0972,"Underfitting due to fewer epochs"
3,t5-base,5,8,5e-5,500,0.01,0.3822,0.1836,0.2961,0.2991,0.8901,0.8650,0.8770,0.1091,"Slight improvement but risk of overfitting"
4,t5-base,3,4,5e-5,500,0.01,0.3702,0.1783,0.2884,0.2911,0.8798,0.8592,0.8691,0.1023,"Smaller batch size led to less stable convergence"
5,t5-base,3,16,5e-5,500,0.01,0.3801,0.1815,0.2931,0.2972,0.8869,0.8639,0.8751,0.1075,"Larger batch size with similar but slightly lower performance"
6,t5-base,3,8,1e-4,500,0.01,0.3754,0.1796,0.2912,0.2933,0.8835,0.8602,0.8715,0.1042,"Higher learning rate led to faster but less optimal convergence"
7,t5-base,3,8,2e-5,500,0.01,0.3768,0.1803,0.2921,0.2955,0.8851,0.8615,0.8731,0.1057,"Lower learning rate improved stability but slower convergence"
8,t5-base,3,8,5e-5,200,0.01,0.3782,0.1808,0.2932,0.2968,0.8865,0.8627,0.8743,0.1067,"Fewer warmup steps"
9,t5-base,3,8,5e-5,1000,0.01,0.3809,0.1820,0.2951,0.2981,0.8887,0.8640,0.8759,0.1082,"More warmup steps with minimal improvement"
10,t5-base,3,8,5e-5,500,0.005,0.3787,0.1810,0.2939,0.2971,0.8873,0.8631,0.8748,0.1071,"Lower weight decay (less regularization)"
11,t5-base,3,8,5e-5,500,0.02,0.3795,0.1815,0.2943,0.2977,0.8879,0.8635,0.8753,0.1076,"Higher weight decay (more regularization)"
12,t5-small,3,8,5e-5,500,0.01,0.3573,0.1687,0.2765,0.2812,0.8691,0.8497,0.8591,0.0936,"Smaller model architecture (baseline comparison)"
13,t5-large,3,4,5e-5,500,0.01,0.3965,0.1921,0.3095,0.3125,0.9017,0.8743,0.8876,0.1167,"Larger model architecture (higher capacity but slower)"
14,t5-base,3,8,5e-5,500,0.01,0.3654,0.1732,0.2832,0.2868,0.8735,0.8572,0.8651,0.0996,"No preprocessing (word normalization/lemmatization)"
15,t5-base,3,8,5e-5,500,0.01,0.3781,0.1802,0.2926,0.2961,0.8861,0.8621,0.8738,0.1063,"Custom preprocessing (extensive stopword removal)"